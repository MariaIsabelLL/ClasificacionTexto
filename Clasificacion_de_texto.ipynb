{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clasificacion de texto.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdzqlY6J2Vb8T+TnNPIUbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaIsabelLL/ClasificacionTexto/blob/main/Clasificacion_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librería NLTK"
      ],
      "metadata": {
        "id": "fWUn2eSdldR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ443VGzGl9v"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#nltk.download('book')\n",
        "\n",
        "#from nltk.book import *\n",
        "\n",
        "#(text1)\n",
        "#for i in range(0,50):\n",
        "#  print(text1[i])\n",
        "#print(len(text1))\n",
        "#print(len(set(text1)))\n",
        "#fdist = FreqDist(text1)\n",
        "#fdist.plot(30, cumulative = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenización"
      ],
      "metadata": {
        "id": "Ak3VzyU5mAGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "frase1 = 'Me gusta este webinar, es genial! Los webinar me gustan mucho.'\n",
        "frase_tokenizada = word_tokenize(frase1)\n",
        "print(frase1)\n",
        "print(frase_tokenizada)\n"
      ],
      "metadata": {
        "id": "D0eu96mBkfGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords y Stemming"
      ],
      "metadata": {
        "id": "erVuvJWPnDGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "\n",
        "stopword_spanish = stopwords.words('spanish')\n",
        "stopword_ingles = stopwords.words('english')\n",
        "print(stopword_spanish)\n",
        "print(stopword_ingles)\n",
        "\n",
        "def removestopwords_ingles(texto):\n",
        "  text_nuevo = [w.lower() for w in texto if w not in stopword_ingles and w not in string.punctuation]\n",
        "  return text_nuevo\n",
        "\n",
        "def removestopwords_espanol(texto):\n",
        "  text_nuevo = [w.lower() for w in texto if w not in stopword_spanish and w not in string.punctuation]\n",
        "  return text_nuevo\n",
        "\n",
        "fdist2 = FreqDist(removestopwords_ingles(text1))\n",
        "fdist2.plot(30, cumulative = False)\n",
        "\n",
        "print(frase_tokenizada)\n",
        "print(removestopwords_espanol(frase_tokenizada))\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stemmed_text = [stemmer.stem(i) for i in removestopwords_espanol(frase_tokenizada)]\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "id": "zbJcgG8xka1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos: movie reviews"
      ],
      "metadata": {
        "id": "yErk8lfRnNHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "print(len(movie_reviews.fileids()))\n",
        "print(len(movie_reviews.categories()))\n",
        "\n",
        "#freqDist = FreqDist(movie_reviews.words())\n",
        "#freqDist.plot(30)\n",
        "\n",
        "new_text = removestopwords_ingles(movie_reviews.words())\n",
        "freqDist2 = FreqDist(new_text)\n",
        "freqDist2.plot(30)\n",
        "print(freqDist2.most_common(30))"
      ],
      "metadata": {
        "id": "yUAbuM1lPu0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento Datos: tokenize y BOW"
      ],
      "metadata": {
        "id": "Vhthpc8tnnXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stemm(words):\n",
        "    words_clean = []    \n",
        "    for word in word_tokenize(words):\n",
        "        word = word.lower()\n",
        "        if word not in string.punctuation and word not in stopword_ingles:\n",
        "            words_clean.append(stemmer.stem(word))\n",
        "    return words_clean\n",
        "\n",
        "def bag_of_words_CountVec(X1):    \n",
        "    matrix_vectorizer = CountVectorizer(tokenizer=remove_stemm,analyzer='word',\n",
        "                            max_features=1000)     \n",
        "    X = matrix_vectorizer.fit_transform(X1).toarray()  \n",
        "    return X,matrix_vectorizer\n",
        "\n",
        "def DatosBOW():\n",
        "    df = pd.DataFrame()\n",
        "    X1 = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
        "    y = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]\n",
        "    print(len(X1))\n",
        "    print('Comentario 1',X1[0])\n",
        "    print('Comentario 2',X1[1])\n",
        "    \n",
        "    (X,count_vectorizer) = bag_of_words_CountVec(X1)\n",
        "    \n",
        "    return X,y,count_vectorizer\n",
        "  \n",
        "(X,y,count_vectorizer) = DatosBOW()\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "\n",
        "def bag_of_words_CountVec2(X1,count_vectorizer):    \n",
        "    X = count_vectorizer.transform(X1).toarray()  \n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "id": "pP8vabWPTENW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clasificador Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "0ZLG7aopngR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def clasificadorBOW(): \n",
        "    (X,y,count_vectorizer) =  DatosBOW()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=0)\n",
        "     \n",
        "    mnb = MultinomialNB()\n",
        "    classifier = mnb.fit(X_train, y_train)    \n",
        "    y_pred = classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(accuracy)\n",
        "\n",
        "    lista_texto = []\n",
        "    lista_texto.append('This is an ugly movie.')\n",
        "    lista_texto.append('This is an excelent movie.')\n",
        "    lista_texto.append('This is an different movie.')\n",
        "    prediccion = classifier.predict(bag_of_words_CountVec2(lista_texto,count_vectorizer)) \n",
        "    print(prediccion)\n",
        "\n",
        "clasificadorBOW()  "
      ],
      "metadata": {
        "id": "58q3zxCRneWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}